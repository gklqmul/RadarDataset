# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.linear_model import LinearRegression
# from sklearn.metrics import mean_absolute_error, mean_squared_error
# from scipy.stats import pearsonr

# from sklearn.ensemble import RandomForestRegressor
# from sklearn.neighbors import KNeighborsRegressor


# class TreeCalibrator:
#     def __init__(self):
#         self.models = []

#     def fit(self, X, y):
#         for i in range(y.shape[1]):
#             model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
#             model.fit(X[:, i].reshape(-1, 1), y[:, i])
#             self.models.append(model)

#     def predict(self, X):
#         adjusted = np.zeros_like(X, dtype=float)
#         for i, model in enumerate(self.models):
#             adjusted[:, i] = model.predict(X[:, i].reshape(-1, 1))
#         return adjusted

#     def evaluate(self, X, y_true):
#         y_pred = self.predict(X)
#         results = []
#         for i in range(y_true.shape[1]):
#             mae = mean_absolute_error(y_true[:, i], y_pred[:, i])
#             rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))
#             mape = np.mean(np.abs((y_true[:, i] - y_pred[:, i]) / np.clip(y_true[:, i], 1e-8, None))) * 100
#             corr, _ = pearsonr(y_true[:, i], y_pred[:, i])
#             results.append((mae, rmse, mape, corr))
#         return y_pred, results
# class KNNCalibrator:
#     def __init__(self, n_neighbors=3):
#         self.n_neighbors = n_neighbors
#         self.models = []

#     def fit(self, X, y):
#         for i in range(y.shape[1]):
#             model = KNeighborsRegressor(n_neighbors=self.n_neighbors)
#             model.fit(X[:, i].reshape(-1, 1), y[:, i])
#             self.models.append(model)

#     def predict(self, X):
#         adjusted = np.zeros_like(X, dtype=float)
#         for i, model in enumerate(self.models):
#             adjusted[:, i] = model.predict(X[:, i].reshape(-1, 1))
#         return adjusted

#     def evaluate(self, X, y_true):
#         y_pred = self.predict(X)
#         results = []
#         for i in range(y_true.shape[1]):
#             mae = mean_absolute_error(y_true[:, i], y_pred[:, i])
#             rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))
#             mape = np.mean(np.abs((y_true[:, i] - y_pred[:, i]) / np.clip(y_true[:, i], 1e-8, None))) * 100
#             corr, _ = pearsonr(y_true[:, i], y_pred[:, i])
#             results.append((mae, rmse, mape, corr))
#         return y_pred, results
# class PolynomialCalibrator:
#     def __init__(self, degree=2):
#         self.degree = degree
#         self.poly = PolynomialFeatures(degree=degree)
#         self.models = []
    
#     def fit(self, X, y):
#         for i in range(y.shape[1]):
#             xi = X[:, i].reshape(-1, 1)
#             yi = y[:, i]
#             xi_poly = self.poly.fit_transform(xi)
#             model = LinearRegression()
#             model.fit(xi_poly, yi)
#             self.models.append(model)
    
#     def predict(self, X):
#         adjusted = np.zeros_like(X, dtype=float)
#         for i, model in enumerate(self.models):
#             xi = X[:, i].reshape(-1, 1)
#             xi_poly = self.poly.transform(xi)
#             adjusted[:, i] = model.predict(xi_poly)
#         return adjusted
    
#     def evaluate(self, X, y_true):
#         y_pred = self.predict(X)
#         results = []
#         for i in range(y_true.shape[1]):
#             mae = mean_absolute_error(y_true[:, i], y_pred[:, i])
#             rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))
#             mape = np.mean(np.abs((y_true[:, i] - y_pred[:, i]) / np.clip(y_true[:, i], 1e-8, None))) * 100
#             corr, _ = pearsonr(y_true[:, i], y_pred[:, i])
#             results.append((mae, rmse, mape, corr))
#         return y_pred, results
       
# def excel_to_nparray(file_path, sheet_name=0, nrows=12, ncols=12):
#     """
#     ä»Excelæ–‡ä»¶è¯»å–12x12è¡¨æ ¼å¹¶è½¬ä¸ºNumPyæ•°ç»„
    
#     å‚æ•°ï¼š
#         file_path: Excelæ–‡ä»¶è·¯å¾„
#         sheet_name: å·¥ä½œè¡¨åæˆ–ç´¢å¼•ï¼ˆé»˜è®¤ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼‰
#         nrows: è¯»å–è¡Œæ•°ï¼ˆé»˜è®¤12ï¼‰
#         ncols: è¯»å–åˆ—æ•°ï¼ˆé»˜è®¤12ï¼‰
    
#     è¿”å›ï¼š
#         12x12çš„NumPyæ•°ç»„
#     """
#     # ä½¿ç”¨pandasè¯»å–Excelï¼ˆä»A1å¼€å§‹ï¼‰
#     df = pd.read_excel(
#         file_path,
#         sheet_name=sheet_name,
#         header=None,       # æ— åˆ—å
#         nrows=nrows,
#         usecols=range(ncols) # è¯»å–å‰12åˆ—
#     )
    
#     # è½¬ä¸ºNumPyæ•°ç»„
#     array = df.to_numpy()
    
#     # éªŒè¯å°ºå¯¸
#     if array.shape != (nrows, ncols):
#         raise ValueError(f"è¯»å–çš„æ•°ç»„å°ºå¯¸ä¸º{array.shape}ï¼Œä¸æ˜¯é¢„æœŸçš„{nrows}x{ncols}")
    
#     return array

# excel_path = "distance&energy.xlsx" 
# data_array = excel_to_nparray(excel_path)
        
# # åˆ†åˆ—
# true_vals = data_array[:, :6]
# diff_arr = np.diff(true_vals, axis=1)  # æŒ‰åˆ—æ±‚å·®
# sum_col = np.sum(diff_arr, axis=1).reshape(-1, 1)  # æŒ‰è¡Œæ±‚å’Œå¹¶è°ƒæ•´å½¢çŠ¶
# true_res = np.hstack((diff_arr, sum_col))
# pred_vals = data_array[:, 6:]
# diff_arr = np.diff(pred_vals, axis=1)  # æŒ‰åˆ—æ±‚å·®
# sum_col = np.sum(diff_arr, axis=1).reshape(-1, 1)  # æŒ‰è¡Œæ±‚å’Œå¹¶è°ƒæ•´å½¢çŠ¶
# pred_res = np.hstack((diff_arr, sum_col))

# pred_TUG = pred_res[:,-1]
# true_TUG = true_res[:,-1]
# pred_TUGs = np.column_stack((pred_TUG[0::3], pred_TUG[1::3], pred_TUG[2::3]))
# true_TUGs = np.column_stack((true_TUG[0::3], true_TUG[1::3], true_TUG[2::3]))
# col1 = np.vstack((pred_TUG[0:3], pred_TUG[6:9]))  # ç¬¬1~3è¡Œ å’Œ ç¬¬7~9è¡Œ
# col2 = np.vstack((pred_TUG[3:6], pred_TUG[9:12])) # ç¬¬4~6è¡Œ å’Œ ç¬¬10~12è¡Œ
# pred_TUGLight = (np.hstack((col1, col2))).T
# col1 = np.vstack((true_TUG[0:3], true_TUG[6:9]))  # ç¬¬1~3è¡Œ å’Œ ç¬¬7~9è¡Œ
# col2 = np.vstack((true_TUG[3:6], true_TUG[9:12])) # ç¬¬4~6è¡Œ å’Œ ç¬¬10~12è¡Œ    
# true_TUGLight = (np.hstack((col1, col2))).T

# pred_person = np.column_stack((pred_TUG[:6],pred_TUG[6:12])) # å–å‰6è¡Œä½œä¸ºè®­ç»ƒé›†
# true_person = np.column_stack((true_TUG[:6],true_TUG[6:12])) # å–å‰6è¡Œä½œä¸ºè®­ç»ƒé›†

# print(true_TUGLight.shape)

# # åˆ†ç»„ï¼šå‰6è¡Œä¸ºè®­ç»ƒï¼Œå6è¡Œä¸ºåº”ç”¨
# true_train = true_person[0::2]    # å–å‰6è¡Œä½œä¸ºè®­ç»ƒé›†
# pred_train = pred_person[0::2]   # å–å‰6è¡Œä½œä¸ºè®­ç»ƒé›†

# true_test = true_person      # æ‰€æœ‰12è¡Œä½œä¸ºæµ‹è¯•é›†
# pred_test = pred_person      # æ‰€æœ‰12è¡Œä½œä¸ºæµ‹è¯•é›†


# # === è®­ç»ƒ + åº”ç”¨ ===
# calibrator = TreeCalibrator()
# calibrator.fit(pred_train, true_train)

# adjusted_test_preds, metrics = calibrator.evaluate(pred_test, true_test)

# # è¾“å‡ºç»“æœ
# print("âœ… åº”ç”¨é˜¶æ®µçš„æ ¡æ­£åé¢„æµ‹å€¼ï¼ˆéšæœºæ£®æ—ï¼‰ï¼š")
# # temp = np.hstack((true_vals, np.round(adjusted_test_preds)))
# # print(temp)

# print("\nğŸ“Š åº”ç”¨é˜¶æ®µæ¯åˆ—çš„ MAE, RMSE, MAPE, Correlationï¼š")
# for i, (mae, rmse, mape, corr) in enumerate(metrics):
#     print(f"åˆ—{i+1} â†’ MAE: {mae/18.1:.2f}, RMSE: {rmse/18.1:.2f}, MAPE: {mape:.2f}%, Correlation: {corr:.4f}")
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy.stats import pearsonr

from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor

import numpy as np

def add_noise_to_diff(data, real_values, sigma):
    """
    å¯¹åŸå§‹æ•°æ®å’ŒçœŸå®å€¼è¿›è¡Œå·®åˆ†ï¼Œå¹¶æ·»åŠ å™ªå£°ï¼Œç”Ÿæˆæ–°çš„å¢å¼ºæ•°æ®å’Œå¢å¼ºçœŸå®å€¼ã€‚
    
    å‚æ•°ï¼š
    data (numpy.ndarray): åŸå§‹æ•°æ®ï¼Œå¤§å°ä¸º (12, 12)ï¼Œå…¶ä¸­å6åˆ—æ˜¯é¢„æµ‹å€¼ã€‚
    real_values (numpy.ndarray): çœŸå®å€¼ï¼Œå¤§å°ä¸º (12, 12)ï¼Œä¸æ•°æ®å¯¹åº”ã€‚
    sigma (float): é«˜æ–¯å™ªå£°çš„æ ‡å‡†å·®ã€‚
    
    è¿”å›ï¼š
    augmented_data (numpy.ndarray): å¢å¼ºåçš„æ•°æ®ï¼Œå¤§å°ä¸º (12, 12)ã€‚
    augmented_real_values (numpy.ndarray): å¢å¼ºåçš„çœŸå®å€¼ï¼Œå¤§å°ä¸º (12, 12)ã€‚
    """
    # è®¡ç®—æ•°æ®çš„å·®åˆ†
    diff_data = np.diff(data, axis=0)
    
    # è®¡ç®—çœŸå®å€¼çš„å·®åˆ†
    diff_real_values = np.diff(real_values, axis=0)
    
    # åœ¨å·®åˆ†å€¼ä¸Šæ·»åŠ å™ªå£°
    noise_data = np.random.normal(0, sigma, diff_data.shape)
    noisy_diff_data = diff_data + noise_data
    
    noise_real_values = np.random.normal(0, sigma, diff_real_values.shape)
    noisy_diff_real_values = diff_real_values + noise_real_values
    
    # ç´¯åŠ ç”Ÿæˆæ–°æ•°æ®
    augmented_data = np.zeros_like(data)
    augmented_data[0, :] = data[0, :]  # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹
    
    augmented_real_values = np.zeros_like(real_values)
    augmented_real_values[0, :] = real_values[0, :]  # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªçœŸå®å€¼
    
    for i in range(1, data.shape[0]):
        augmented_data[i, :] = augmented_data[i - 1, :] + noisy_diff_data[i - 1, :]
        augmented_real_values[i, :] = augmented_real_values[i - 1, :] + noisy_diff_real_values[i - 1, :]
    
    return augmented_data, augmented_real_values

def generate_augmented_data(original_data, original_real_values, sigma, num_augmented_samples=3):
    """
    ç”Ÿæˆå¢å¼ºåçš„æ•°æ®é›†ï¼Œé€šè¿‡å¤šæ¬¡åº”ç”¨æ•°æ®å¢å¼ºè¿‡ç¨‹æ¥å¢åŠ æ•°æ®é‡ã€‚
    
    å‚æ•°ï¼š
    original_data (numpy.ndarray): åŸå§‹æ•°æ®ï¼Œå¤§å°ä¸º (12, 12)ã€‚
    original_real_values (numpy.ndarray): çœŸå®å€¼ï¼Œå¤§å°ä¸º (12, 12)ã€‚
    sigma (float): é«˜æ–¯å™ªå£°çš„æ ‡å‡†å·®ã€‚
    num_augmented_samples (int): ç”Ÿæˆå¢å¼ºæ•°æ®çš„æ•°é‡ï¼ˆå¢åŠ æ•°æ®é‡çš„å€æ•°ï¼‰ã€‚
    
    è¿”å›ï¼š
    augmented_data (numpy.ndarray): å¢å¼ºåçš„æ•°æ®ï¼Œå¤§å°ä¸º (12 * num_augmented_samples, 12)ã€‚
    augmented_real_values (numpy.ndarray): å¢å¼ºåçš„çœŸå®å€¼ï¼Œå¤§å°ä¸º (12 * num_augmented_samples, 12)ã€‚
    """
    augmented_data_list = []
    augmented_real_values_list = []
    
    for _ in range(num_augmented_samples):
        augmented_data, augmented_real_values = add_noise_to_diff(original_data, original_real_values, sigma)
        augmented_data_list.append(augmented_data)
        augmented_real_values_list.append(augmented_real_values)
    
    # å°†å¢å¼ºçš„æ•°æ®åˆå¹¶
    augmented_data = np.vstack(augmented_data_list)
    augmented_real_values = np.vstack(augmented_real_values_list)
    
    return augmented_data, augmented_real_values


class TreeCalibrator:
    def __init__(self):
        self.models = []

    def fit(self, X, y):
        for i in range(y.shape[1]):
            model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
            model.fit(X[:, i].reshape(-1, 1), y[:, i])
            self.models.append(model)

    def predict(self, X):
        adjusted = np.zeros_like(X, dtype=float)
        for i, model in enumerate(self.models):
            adjusted[:, i] = model.predict(X[:, i].reshape(-1, 1))
        return adjusted

    def evaluate(self, X, y_true):
        y_pred = self.predict(X)
        results = []
        for i in range(y_true.shape[1]):
            mae = mean_absolute_error(y_true[:, i], y_pred[:, i])
            rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))
            mape = np.mean(np.abs((y_true[:, i] - y_pred[:, i]) / np.clip(y_true[:, i], 1e-8, None))) * 100
            corr, _ = pearsonr(y_true[:, i], y_pred[:, i])
            results.append((mae, rmse, mape, corr))
        return y_pred, results


# æ•°æ®åŠ è½½å‡½æ•°
def excel_to_nparray(file_path, sheet_name=0, nrows=12, ncols=12):
    """
    ä»Excelæ–‡ä»¶è¯»å–12x12è¡¨æ ¼å¹¶è½¬ä¸ºNumPyæ•°ç»„
    
    å‚æ•°ï¼š
        file_path: Excelæ–‡ä»¶è·¯å¾„
        sheet_name: å·¥ä½œè¡¨åæˆ–ç´¢å¼•ï¼ˆé»˜è®¤ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼‰
        nrows: è¯»å–è¡Œæ•°ï¼ˆé»˜è®¤12ï¼‰
        ncols: è¯»å–åˆ—æ•°ï¼ˆé»˜è®¤12ï¼‰
    
    è¿”å›ï¼š
        12x12çš„NumPyæ•°ç»„
    """
    # ä½¿ç”¨pandasè¯»å–Excelï¼ˆä»A1å¼€å§‹ï¼‰
    df = pd.read_excel(
        file_path,
        sheet_name=sheet_name,
        header=None,       # æ— åˆ—å
        nrows=nrows,
        usecols=range(ncols) # è¯»å–å‰12åˆ—
    )
    
    # è½¬ä¸ºNumPyæ•°ç»„
    array = df.to_numpy()
    
    # éªŒè¯å°ºå¯¸
    if array.shape != (nrows, ncols):
        raise ValueError(f"è¯»å–çš„æ•°ç»„å°ºå¯¸ä¸º{array.shape}ï¼Œä¸æ˜¯é¢„æœŸçš„{nrows}x{ncols}")
    
    return array


# LOOCVå®ç°
def loocv(X, y, calibrator_class):
    n_samples = X.shape[0]
    all_metrics = []

    for i in range(0, n_samples, 2):  # æ¯æ¬¡è·³è¿‡ä¸¤ä¸ªæ ·æœ¬
        # ç•™å‡º2ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
        X_train = np.delete(X, range(i, i+2), axis=0)
        y_train = np.delete(y, range(i, i+2), axis=0)
        X_test = X[i:i+2, :]
        y_test = y[i:i+2, :]

        # åˆå§‹åŒ–æ ¡å‡†å™¨å¹¶è®­ç»ƒ
        calibrator = calibrator_class()
        calibrator.fit(X_train, y_train)

        # è¯„ä¼°æ¨¡å‹
        _, metrics = calibrator.evaluate(X_test, y_test)
        all_metrics.append(metrics)

    # æ±‡æ€»ç»“æœ
    avg_metrics = np.mean(all_metrics, axis=0)
    return avg_metrics

def kfold_cv(X, y, model_class, k=5):
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    all_metrics = []

    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # è®­ç»ƒæ¨¡å‹
        model = model_class()
        model.fit(X_train, y_train)
        
        # é¢„æµ‹å¹¶è¯„ä¼°
        preds, metrics = model.evaluate(X_test, y_test)
        all_metrics.append(metrics)
    
    # æ±‡æ€»æ‰€æœ‰çš„è¯„ä¼°æŒ‡æ ‡
    avg_metrics = np.mean(all_metrics, axis=0)
    
    return avg_metrics

# ä¸»å‡½æ•°
if __name__ == "__main__":
    excel_path = "distance&energy.xlsx"
    data_array = excel_to_nparray(excel_path)
    sigma = 0.05  # é«˜æ–¯å™ªå£°çš„æ ‡å‡†å·®
    true_vals = data_array[:, :6]
    pred_vals = data_array[:, 6:]
    # ç”Ÿæˆå¢å¼ºåçš„æ•°æ®é›†ï¼Œå¢åŠ åˆ° 36x12
    augmented_data, augmented_real_values = generate_augmented_data(pred_vals, true_vals, sigma, num_augmented_samples=3)
    # åˆ†åˆ—
    diff_arr = np.diff(augmented_real_values, axis=1)  # æŒ‰åˆ—æ±‚å·®
    sum_col = np.sum(diff_arr, axis=1).reshape(-1, 1)  # æŒ‰è¡Œæ±‚å’Œå¹¶è°ƒæ•´å½¢çŠ¶
    true_res = np.hstack((diff_arr, sum_col))

    diff_arr = np.diff(augmented_data, axis=1)  # æŒ‰åˆ—æ±‚å·®
    sum_col = np.sum(diff_arr, axis=1).reshape(-1, 1)  # æŒ‰è¡Œæ±‚å’Œå¹¶è°ƒæ•´å½¢çŠ¶
    pred_res = np.hstack((diff_arr, sum_col))
#    pred_TUGspeed = np.column_stack((pred_TUG[0::3], pred_TUG[1::3], pred_TUG[2::3]))
#     true_TUGspeed = np.column_stack((true_TUG[0::3], true_TUG[1::3], true_TUG[2::3]))
    pred_TUG_2d = pred_res[:,-1]
    true_TUG_2d = true_res[:,-1]
    pred_TUG = pred_TUG_2d.reshape(-1, 1)
    true_TUG = true_TUG_2d.reshape(-1, 1)
    col1 = np.vstack((pred_TUG[0:3],pred_TUG[6:9],pred_TUG[12:15], pred_TUG[18:21], pred_TUG[24:27], pred_TUG[30:33])) 

    col2 = np.vstack((pred_TUG[3:6], pred_TUG[9:12], pred_TUG[15:18], pred_TUG[21:24], pred_TUG[27:30], pred_TUG[33:36]))  
    pred_TUGLight = np.hstack((col1, col2))
    col11 = np.vstack((true_TUG[0:3],  true_TUG[6:9],   true_TUG[12:15], 
                  true_TUG[18:21], true_TUG[24:27], true_TUG[30:33])) 

    col21 = np.vstack((true_TUG[3:6],   true_TUG[9:12],  true_TUG[15:18], 
                  true_TUG[21:24], true_TUG[27:30], true_TUG[33:36])) 
    true_TUGLight = np.hstack((col11, col21))

    pred_person = np.column_stack((pred_TUG[:6],pred_TUG[6:12])) # å–å‰6è¡Œä½œä¸ºè®­ç»ƒé›†
    true_person = np.column_stack((true_TUG[:6],true_TUG[6:12])) # å–å‰6è¡Œä½œä¸ºè®­ç»ƒé›†

    
    metrics = kfold_cv(pred_TUGLight, true_TUGLight, TreeCalibrator, k=5)

    # è¾“å‡ºæ¯åˆ—çš„å¹³å‡æ€§èƒ½æŒ‡æ ‡
    print("\nğŸ“Š æ¯åˆ—çš„å¹³å‡ MAE, RMSE, MAPE, Correlationï¼š")
    for i, (mae, rmse, mape, corr) in enumerate(metrics):
        print(f"åˆ—{i+1} â†’ MAE: {mae/18.1:.2f}, RMSE: {rmse/18.1:.2f}, MAPE: {mape:.2f}%, Correlation: {corr:.4f}")
